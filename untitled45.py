# -*- coding: utf-8 -*-
"""Untitled45.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P9kHV5FsFIETelPUU2R-7SZvDCsf0edn
"""

import pandas as pd

# Load the dataset
file_path = '/content/diabetes.csv'
df = pd.read_csv(file_path)

# Display the first few rows of the dataset
df.head()

# Load useful libraries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report

# Separate Features and Target Variables
X = df.drop(columns='Outcome')
y = df['Outcome']

# Create Train & Test Data
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,
                                                	stratify =y,
                                                	random_state = 13)

# Build the model
rf_clf = RandomForestClassifier(max_features=2, n_estimators =100 ,bootstrap = True)

rf_clf.fit(X_train, y_train)

# Make prediction on the testing data
y_pred = rf_clf.predict(X_test)

# Classification Report
print(classification_report(y_pred, y_test))

!pip install shap

# Print feature importances
feature_importance = rf_clf.feature_importances_
feature_names = X.columns
for importance, name in sorted(zip(feature_importance, feature_names), reverse=True):
    print(f"{name}: {importance}")

"""#SHAP ON PIMA DATASET"""

import shap
import matplotlib.pyplot as plt

# load JS visualization code to notebook
shap.initjs()

# Create the explainer
explainer = shap.TreeExplainer(rf_clf)

shap_values = explainer.shap_values(X)

import shap
import matplotlib.pyplot as plt

print("Variable Importance Plot - Global Interpretation")
print("SHAP values shape:", shap_values.shape)
print("X shape:", X.shape)

# For binary classification, use the SHAP values for one class
shap_values_class1 = shap_values[:, :, 1]  # Assuming shape (num_samples, num_features, num_classes)

# Create the summary plot
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values_class1, X)
plt.tight_layout()
plt.show()

import shap
import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 6))

# Plot for Class 0
shap.summary_plot(shap_values[:, :, 0], X, plot_type="bar", show=False)
plt.gca().set_title("Class 0")
plt.gca().set_position([0, 0, 0.5, 1])  # Adjust position for ax1

# Plot for Class 1
shap.summary_plot(shap_values[:, :, 1], X, plot_type="bar", show=False)
plt.gca().set_title("Class 1")
plt.gca().set_position([0.5, 0, 0.5, 1])  # Adjust position for ax2

plt.tight_layout()
plt.show()

# import shap
# import matplotlib.pyplot as plt

# # Create a figure with two subplots side by side
# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# # Plot for Class 0
# plt.sca(ax1)
# shap.summary_plot(shap_values[:, :, 0], X, plot_type="bar", show=False)
# ax1.set_title("Class 0")
# ax1.set_xlabel("mean(|SHAP value|) (average impact on model output magnitude)")

# # Plot for Class 1
# plt.sca(ax2)
# shap.summary_plot(shap_values[:, :, 1], X, plot_type="bar", show=False)
# ax2.set_title("Class 1")
# ax2.set_xlabel("mean(|SHAP value|) (average impact on model output magnitude)")

# # Adjust layout and display the plot
# plt.tight_layout()
# plt.show()

# import shap
# import matplotlib.pyplot as plt
# import numpy as np

# # 1. Beeswarm plot
# plt.figure(figsize=(12, 8))
# shap.summary_plot(shap_values, X, plot_type="dot", class_names=["Class 0", "Class 1"])
# plt.tight_layout()
# plt.show()

# # 2. Difference in SHAP values
# shap_diff = shap_values[:, :, 1] - shap_values[:, :, 0]
# plt.figure(figsize=(10, 8))
# shap.summary_plot(shap_diff, X, plot_type="bar")
# plt.title("Difference in SHAP values (Class 1 - Class 0)")
# plt.tight_layout()
# plt.show()

# # 3. Absolute values for Class 1
# plt.figure(figsize=(10, 8))
# shap.summary_plot(np.abs(shap_values[:, :, 1]), X, plot_type="bar")
# plt.title("Absolute SHAP values for Class 1")
# plt.tight_layout()
# plt.show()

import shap
import numpy as np
import matplotlib.pyplot as plt

# Compute the mean absolute SHAP values across classes
shap_values_mean = np.abs(shap_values).mean(axis=2)

# Plot the average SHAP values
plt.figure(figsize=(8, 6))
shap.summary_plot(shap_values_mean, X, plot_type="bar")
plt.tight_layout()
plt.show()

# Split the data into features and target
X = df.iloc[:, :-1]  # All columns except the last one
y = df.iloc[:, -1]   # The last column is the target

print(y)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}')

# Classification Report
from sklearn.metrics import classification_report
print(classification_report(y_pred, y_test))

"""#LIME ON PIMA DATASET"""

!pip install lime

import lime
import lime.lime_tabular

# Create a LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X.columns, class_names=['Non-Diabetic', 'Diabetic'], discretize_continuous=True)

# Explain the prediction for the first instance in the test set
i = 0 # You can change this to explain other instances
exp = explainer.explain_instance(X_test.values[i], model.predict_proba, num_features=10)

# Show the explanation
exp.show_in_notebook(show_all=False)